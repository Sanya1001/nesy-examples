// Input from neural networks
type grid_node(x: i8, y: i8)
type is_agent(x: i8, y: i8)
type is_goal(x: i8, y: i8)
type is_enemy(x: i8, y: i8)

// Constants
const UP = 0, RIGHT = 1, DOWN = 2, LEFT = 3

// Basic connectivity
rel node(x, y) = grid_node(x, y), not is_enemy(x, y)
rel edge(x, y, x, yp, UP) = node(x, y), node(x, yp), yp == y + 1
rel edge(x, y, xp, y, RIGHT) = node(x, y), node(xp, y), xp == x + 1
rel edge(x, y, x, yp, DOWN) = node(x, y), node(x, yp), yp == y - 1
rel edge(x, y, xp, y, LEFT) = node(x, y), node(xp, y), xp == x - 1

// Path for connectivity; will condition on no enemy on the path
rel path(x, y, x, y) = node(x, y)
rel path(x, y, xp, yp) = edge(x, y, xp, yp, _)
rel path(x, y, xpp, ypp) = path(x, y, xp, yp), edge(xp, yp, xpp, ypp, _)

// Get the next position
rel next_position(a, xp, yp) = is_agent(x, y), edge(x, y, xp, yp, a)
rel expected_reward(a) = next_position(a, x, y), is_goal(gx, gy), path(x, y, gx, gy)

// ============ EXAMPLE ============

// The following example denotes the following arena
//
// g - -
// E E -
// a - -
//
// where "g" is the goal and "a" is the current position.
// The agent needs to avoid the enemies ("E")
// and therefore the best action is to go RIGHT (represented by integer 1)

rel grid_node = {
  0.95::(0, 2), 0.95::(1, 2), 0.95::(2, 2),
  0.95::(0, 1), 0.95::(1, 1), 0.95::(2, 1),
  0.95::(0, 0), 0.95::(1, 0), 0.95::(2, 0),
}

rel is_enemy = {0.99::(0, 1), 0.99::(1, 1)}

rel is_agent = {0.99::(0, 0)}

rel is_goal = {0.98::(0, 2)}

query expected_reward
